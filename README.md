# MNIST Digit Classification using MLP (TensorFlow & PyTorch)

Ce projet a pour objectif de construire un **Perceptron Multicouche (MLP)** pour classifier les chiffres manuscrits du cÃ©lÃ¨bre **dataset MNIST**, en utilisant deux frameworks populaires : **TensorFlow/Keras** et **PyTorch**.

---

## ðŸ§  Objectif

Classer une image de chiffre manuscrit (28x28 pixels en niveaux de gris) en une des 10 classes possibles (0 Ã  9).


---

## ðŸ“¦ Dataset

Le projet utilise le dataset **MNIST** :

- 60 000 images pour lâ€™entraÃ®nement
- 10 000 images pour le test
- Chaque image est de taille **28x28 pixels** en niveaux de gris
- 10 classes : **chiffres de 0 Ã  9**

Les images sont automatiquement tÃ©lÃ©chargÃ©es via `tf.keras.datasets.mnist` ou `torchvision.datasets.MNIST`.

---

## âš™ï¸ Ã‰tapes du projet

### 1. ðŸ“¥ Chargement des donnÃ©es

- Normalisation des pixels (valeurs entre 0 et 1)
- Redimensionnement : aplatissement en vecteurs de taille **784 (28x28)**

### 2. ðŸ—ï¸ Architecture du MLP

Architecture commune utilisÃ©e dans **TensorFlow** et **PyTorch** :

Input: Flatten(28x28 â†’ 784)
Dense 1: 128 neurones + ReLU
Dropout: 20%
Dense 2: 256 neurones + ReLU
Dense 3: 128 neurones + ReLU
Output: 10 neurones + Softmax

> ðŸ” Activation utilisÃ©e : **ReLU**
>  
> ðŸ§  Sortie finale : **Softmax** sur 10 classes

### 3. ðŸ§ª EntraÃ®nement

- **Fonction de perte** :
  - `categorical_crossentropy` (TensorFlow)
  - `CrossEntropyLoss` (PyTorch)
- **Optimiseur** : `Adam`
- **MÃ©trique** : `accuracy`
- **Ã‰poques** : 10
- **Batch size** : 32 ou 64

### 4. ðŸ“Š Ã‰valuation

- Ã‰valuation sur lâ€™ensemble de test
- Affichage de la prÃ©cision finale
- Optionnel : affichage de quelques prÃ©dictions correctes/incorrectes

---

## ðŸ“‰ RÃ©sultats attendus

- PrÃ©cision attendue : **â‰¥ 97%** sur lâ€™ensemble de test
- Affichage optionnel de quelques images avec leur prÃ©diction
